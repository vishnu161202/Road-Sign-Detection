{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b4b123-af2c-4202-82dc-8f564891b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8da9693-458f-4832-b854-6ce20a1204b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "547128b9-d5b6-448c-913f-137f31afdf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths to the image and annotation folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df9a8846-e6b5-4613-9305-549494b4ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'C:\\\\Users\\\\Vishnu\\\\Downloads\\\\archive\\\\images'\n",
    "annotation_folder = 'C:\\\\Users\\\\Vishnu\\\\Downloads\\\\archive\\\\annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d73b59f-e70b-4617-92cd-5096030b759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of image and annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2145a047-80ad-4a1d-9cd7-4bcf95854fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = sorted(os.listdir(image_folder))\n",
    "annotation_files = sorted(os.listdir(annotation_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fbd1b03-0ea6-4731-ab92-442a642e5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b27eaf0-9031-42c6-81c6-e634842cb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2030c9d-b99c-4ade-9e74-8378d1f9d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8c124e1-52ec-471a-8363-4fd069c1afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da97327b-e8d4-47c7-95fe-66fff1c5f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc63a499-855b-41af-8546-b89785bd6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotation_file in annotation_files:\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(os.path.join(annotation_folder, annotation_file))\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Get the image file name\n",
    "    image_file_name = root.find('filename').text\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(os.path.join(image_folder, image_file_name))\n",
    "\n",
    "    # Resize the image\n",
    "    image = cv2.resize(image, image_size)\n",
    "\n",
    "    # Preprocess the image (e.g., normalize, etc.)\n",
    "    # ...\n",
    "\n",
    "    # Append the preprocessed image to the list\n",
    "    images.append(image)\n",
    "\n",
    "    # Get the object label\n",
    "    object_label = root.find('object').find('name').text\n",
    "\n",
    "    # Map the object label to the corresponding class index\n",
    "    if object_label == 'trafficlight':\n",
    "        label = 0\n",
    "    elif object_label == 'stop':\n",
    "        label = 1\n",
    "    elif object_label == 'speedlimit':\n",
    "        label = 2\n",
    "    elif object_label == 'crosswalk':\n",
    "        label = 3\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Append the label to the list\n",
    "    labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bf4cf-bc11-41ac-92a2-df0c8846c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a46839ec-8bd9-41cf-be9b-cfd6534564ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa20b89e-fd20-488a-b654-323e6c9329cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee2882dc-997e-4add-83f3-fd0f8ea89583",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0614952-61cc-4c18-9bc8-70489dcff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e8dd7e8-add6-47bd-a399-5e89cb1429d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2eacf9bd-5d97-4ca1-bcd2-5b6cbbceaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to categorical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93f41454-2eb9-4bbe-b632-ebba15aef075",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c6dc3a0-5707-44e9-9be4-8af56f5bd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5e4a9e3-ef71-4bc7-bbed-bfc012d6cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=X_train[0].shape))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa1d2344-fadb-45aa-972c-1e609d992719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11ef8ad0-1f25-47a6-a098-0ca243314917",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1c1d2-7bf9-4e49-b257-00f99c7d4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75a75be9-7a6a-49ed-b22a-6b3ef556665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 3s 103ms/step - loss: 0.8780 - accuracy: 0.7290 - val_loss: 0.9272 - val_accuracy: 0.6818\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 1s 61ms/step - loss: 0.7566 - accuracy: 0.7646 - val_loss: 0.8806 - val_accuracy: 0.7102\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.7094 - accuracy: 0.7817 - val_loss: 0.8417 - val_accuracy: 0.7159\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 2s 84ms/step - loss: 0.6345 - accuracy: 0.7817 - val_loss: 1.0339 - val_accuracy: 0.7102\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 2s 82ms/step - loss: 0.5667 - accuracy: 0.8017 - val_loss: 0.7948 - val_accuracy: 0.7330\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.5248 - accuracy: 0.8146 - val_loss: 0.7781 - val_accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 2s 72ms/step - loss: 0.4651 - accuracy: 0.8374 - val_loss: 0.7660 - val_accuracy: 0.7614\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.4259 - accuracy: 0.8573 - val_loss: 0.8753 - val_accuracy: 0.7216\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 2s 71ms/step - loss: 0.3261 - accuracy: 0.8930 - val_loss: 0.8674 - val_accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.2813 - accuracy: 0.9116 - val_loss: 0.9751 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c5b5c0e20>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4999600-a6b6-477c-845a-b96c4589a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ee94f31-ede0-4f4a-88a6-5b107241c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Vishnu\\Downloads\\Road-sign\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Vishnu\\Downloads\\Road-sign\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('C:\\\\Users\\\\Vishnu\\\\Downloads\\\\Road-sign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae302e19-9d6a-45bc-b091-c5ef39bc010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e09d4fb6-7560-4621-8545-bbcb169bda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('C:\\\\Users\\\\Vishnu\\\\Downloads\\\\Road-sign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b53ef226-a361-4545-8550-72b5bc20c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fa8b225-d835-4954-80af-dd8e69d7bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = cv2.imread('C:\\\\Users\\\\Vishnu\\\\Downloads\\\\archive\\\\images\\\\road500.png')\n",
    "preprocessed_image = cv2.resize(input_image, image_size) / 255.0\n",
    "preprocessed_image = np.expand_dims(preprocessed_image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "849463ce-1da6-434e-9bad-55c298e80382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc8dcf31-cbb5-43d7-8850-54848a352d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = loaded_model.predict(preprocessed_image)\n",
    "predicted_label_index = np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1868be21-ddb1-4f67-9ec6-9185d99f3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the predicted label index to the corresponding class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7fd458e-b2d4-44aa-8408-005bb27ff7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['Traffic Light', 'Stop', 'Speed Limit', 'Crosswalk']\n",
    "predicted_label = class_labels[predicted_label_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f71877a-2631-43ea-a4d8-7ae971774266",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Print the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "701f81a4-2155-4cb6-98f9-b54a6206e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Speed Limit\n"
     ]
    }
   ],
   "source": [
    "print('Predicted Label:', predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9e920-6323-49e8-bb44-7063fa630f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
